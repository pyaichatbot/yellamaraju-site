---
title: "Before You Build: A Realistic Framework for Evaluating AI Use Cases"
description: "Why 80% of AI projects fail‚Äîand how to avoid being one of them. A practitioner's framework for evaluating AI use cases before you write a single line of code."
date: 2026-01-06
tags: ["AI", "Architecture", "Best Practices"]
---

import Callout from '../../components/Callout.astro';
import Mermaid from '../../components/Mermaid.astro';

Imagine this scenario: At 2 AM on a Tuesday, a team gets a call. Their AI-powered fraud detection system has flagged 40% of legitimate transactions as fraudulent. Customers are furious. The system has been in production for three months, and they've just discovered a fundamental flaw: they'd never properly validated whether AI was the right solution.

That night cost them ‚Ç¨50K in lost revenue and three months of development time. The lesson? **Most AI projects fail not because the technology is wrong, but because the use case evaluation is wrong.**

McKinsey reports that 55% of organizations pilot AI, but only 20% deploy to production. Why? They skip the assessment phase. They jump to building before asking "Should we build this?"

Here's the framework I wish we'd used‚Äîand the one I now apply to every AI initiative before writing a single line of code.

<Callout type="warning" title="The Real Question">
This isn't about "How do we build an AI/ML model?" It's about "Does this problem actually NEED AI? And if yes, what LEVEL of AI?"
</Callout>

## The Three Failures That Kill AI Projects

Before diving into the framework, let's understand why AI projects fail. I've seen three patterns repeat across dozens of initiatives:

### 1. Bad Problem Statement

**The Symptom:** "We want to use AI for customer support."

**Why It Fails:** This isn't a problem‚Äîit's a solution looking for a problem. What's the actual pain? Long response times? High ticket volume? Poor resolution rates?

**The Fix:** Start with the business problem, not the technology.

### 2. Wrong Abstraction Level

**The Symptom:** Building a Level 4 (Advanced ML) system when Level 1 (Rules) would work.

**Why It Fails:** Over-engineering. A simple rule-based system catches 85% of cases, but teams jump to deep learning "because AI is cool."

**The Fix:** Match the AI level to the problem complexity.

### 3. Wrong Expectations

**The Symptom:** Expecting 100% accuracy from day one.

**Why It Fails:** AI systems are probabilistic. They improve over time. Teams abandon projects when initial accuracy is 75% instead of 95%.

**The Fix:** Set realistic success criteria based on baseline performance.

<Callout type="success" title="Real Example">
Our fraud detection system started at 85% accuracy with simple Logistic Regression. After two years of iteration, it's now at 99.2% with ensemble models. But we almost killed it in month three because "85% wasn't good enough." The lesson: start simple, improve iteratively.
</Callout>

## The 3-Dimensional Assessment Framework

Every AI use case must pass three dimensions: **Desirability**, **Feasibility**, and **Viability**. Fail any dimension, and the project should stop or pivot.

<Mermaid chart={`
graph TD
    A[AI Use Case Idea] --> B[Desirability Check]
    B -->|Pass| C[Feasibility Check]
    B -->|Fail| D[STOP: No Business Value]
    C -->|Pass| E[Viability Check]
    C -->|Fail| F[STOP: Can't Build It]
    E -->|Pass| G[Proceed to AI Level Selection]
    E -->|Fail| H[STOP: Can't Sustain It]
`} />

### Dimension 1: Desirability ‚Äî Is the Problem Worth Solving?

**Question:** Would solving this problem create measurable business value?

**What to Assess:**

1. **Quantified Impact**
   - What's the current cost of the problem? (Time, money, errors)
   - What's the cost of doing nothing?
   - What's the measurable improvement we need?

2. **Strategic Alignment**
   - Does this align with business priorities?
   - Is there executive sponsorship?
   - Will users actually adopt this?

3. **Success Metrics**
   - How will we measure success?
   - What's the baseline performance today?
   - What improvement justifies the investment?

**Example: Fraud Detection**

```
Problem: Fraudulent transactions slip through our rule-based system
Current State: 2.8% fraud rate, costs ‚Ç¨5M annually
Target State: Reduce to &lt;0.8% fraud rate (‚Ç¨3M savings)
Baseline: Manual rules catch 1.8% fraud
Success Metric: Fraud catch rate >99%, false positives &lt;0.5%
```

**Red Flags:**
- ‚ùå Vague problem statement ("improve customer experience")
- ‚ùå No baseline metrics
- ‚ùå No clear business owner
- ‚ùå Success criteria are subjective

**Green Lights:**
- ‚úÖ Quantified current cost
- ‚úÖ Clear target improvement
- ‚úÖ Measurable success metrics
- ‚úÖ Business owner identified

### Dimension 2: Feasibility ‚Äî Can We Technically Do This?

**Question:** Do we have the data, skills, and infrastructure to build this?

**What to Assess:**

1. **Data Reality Check**
   - Do we have the data we need? (Not "can we collect it"‚Äîdo we HAVE it?)
   - Is the data labeled? Complete? Fresh?
   - How much historical data exists?
   - What's the data quality?

2. **Technical Fit**
   - Does this problem require AI, or would rules/heuristics work?
   - Do we have the technical skills in-house?
   - Can we integrate with existing systems?
   - Are there compliance/regulatory constraints?

3. **Data Access & Governance**
   - Can we legally use this data?
   - Do we have privacy/compliance approval?
   - Who owns the data, and will they give us access?

**Example: Customer Churn Prediction**

```
Data Needed: 6-12 months of customer behavior + churn labels
Data We Have: ‚úÖ Yes, 6-12 months per customer
Labels: ‚úÖ Yes, but...
Issue: Some regions have only 3 months of history
Issue: Definition of "churn" varies by product
Decision: DATA EXISTS, but quality needs validation (PoC risk)
```

**Example: Merchant Category Code Automation**

```
Data Needed: Merchant records with correct category codes
Data We Have: ‚úÖ Yes, 200K merchant records
Issue: Historical data is 80% correct (20% wrong categories)
Issue: No machine-readable explanations of why merchants get certain codes
Decision: Can't train ML on 80% correct labels. STOP or pivot to rules + GenAI
```

**Red Flags:**
- ‚ùå Data doesn't exist (only "we could collect it")
- ‚ùå Data quality &lt;70% (too many missing values, errors)
- ‚ùå No labeled training data
- ‚ùå Compliance blockers (GDPR, industry regulations)
- ‚ùå Data locked in vendor systems we don't control

**Green Lights:**
- ‚úÖ Data exists and is accessible
- ‚úÖ Data quality >85%
- ‚úÖ Labeled training data available
- ‚úÖ Compliance approval obtained
- ‚úÖ Technical team has required skills

### Dimension 3: Viability ‚Äî Can We Sustain This?

**Question:** Is this financially justified and operationally sustainable?

**What to Assess:**

1. **ROI Calculation**
   - Annual benefit: What will be saved/earned?
   - Implementation cost: What will it cost to build?
   - Operating cost: Ongoing maintenance/infrastructure
   - Payback period: When does it break even?

2. **Team & Skills**
   - Do we have the right team?
   - Can we maintain this long-term?
   - What training is needed?

3. **Change Management**
   - Will users adopt this?
   - What process changes are required?
   - Is the organization ready?

**Example: Fraud Detection ROI**

```
Year 1 (Implementation):
‚îú‚îÄ Implementation cost: ‚Ç¨600K
‚îú‚îÄ Infrastructure cost: ‚Ç¨200K
‚îú‚îÄ Team cost (2 FTE): ‚Ç¨250K
‚îî‚îÄ Total Year 1 cost: ‚Ç¨1,050K

Annual Benefit (Ongoing):
‚îú‚îÄ Fraud reduction: ‚Ç¨2M/year (0.8% rate instead of 2.8%)
‚îú‚îÄ Manual review savings: ‚Ç¨300K/year
‚îî‚îÄ Total annual benefit: ‚Ç¨2.3M/year

Payback Period: Year 1 = -‚Ç¨1,050K + ‚Ç¨2.3M = +‚Ç¨1.25M
‚Üí Positive in Year 1. ‚úÖ GO

Risk Scenario (50% as good):
‚îú‚îÄ Fraud reduction: ‚Ç¨1M/year
‚îú‚îÄ Manual review savings: ‚Ç¨150K/year
‚îú‚îÄ Total benefit: ‚Ç¨1.15M/year
‚îú‚îÄ Payback: 0.9 years
‚Üí Still positive. ‚úÖ GO
```

**Red Flags:**
- ‚ùå Payback period >2 years
- ‚ùå ROI is negative even in best case
- ‚ùå No budget for ongoing operations
- ‚ùå Team doesn't have skills (and can't acquire them)
- ‚ùå Users are resistant to change

**Green Lights:**
- ‚úÖ Positive ROI in Year 1
- ‚úÖ Payback period &lt;18 months
- ‚úÖ Budget approved for build and operations
- ‚úÖ Team has or can acquire skills
- ‚úÖ Users are engaged and supportive

## The 5 Levels of AI: From Analytics to Agentic AI

Not all AI is created equal. Understanding AI levels helps you pick the right solution for your problem‚Äîand avoid over-engineering.

<Mermaid chart={`
graph LR
    A[Level 0:<br/>No AI Needed] --> B[Level 1:<br/>Analytics/BI]
    B --> C[Level 2:<br/>AI-Supported]
    C --> D[Level 3:<br/>AI-Integrated]
    D --> E[Level 4:<br/>Advanced ML]
    E --> F[Level 5:<br/>Agentic AI]
    
    style A fill:#ffcccc
    style B fill:#ffffcc
    style C fill:#ccffcc
    style D fill:#ccffff
    style E fill:#ccccff
    style F fill:#ffccff
`} />

### Level 0: This Doesn't Need AI

**What It Is:** Rule-based logic, simple automation, heuristics

**Examples:**
- "If transaction amount > ‚Ç¨5,000, flag for review"
- "Route ticket to team based on category"
- "Send email when status changes"

**When to Use:**
- Problem is deterministic (same input ‚Üí same output)
- Rules can capture all cases
- No pattern recognition needed

**Cost:** ‚Ç¨5K-20K | **Time:** 2 weeks

**Real Example:** We built a rule-based system for routing support tickets. It worked perfectly. No AI needed.

### Level 1: Traditional Data Analytics / BI

**What It Is:** Statistical models, regression, basic analytics

**Examples:**
- Sales forecasting with linear regression
- Customer segmentation with clustering
- Trend analysis with time series

**When to Use:**
- Linear relationships in data
- Historical patterns predict future
- No real-time decisions needed

**Cost:** ‚Ç¨20K-50K | **Time:** 3 weeks

### Level 2: AI-Supported (Human Decision-Making Enhanced)

**What It Is:** AI suggests, human decides

**Examples:**
- AI recommends products, human approves
- AI flags anomalies, human investigates
- AI generates content, human edits

**When to Use:**
- Human judgment is critical
- AI improves efficiency but can't replace humans
- Error tolerance is low

**Cost:** ‚Ç¨50K-150K | **Time:** 4-6 weeks

**Real Example:** Our customer churn prediction model (Level 2) identifies at-risk customers. The retention team decides which offers to send. AI supports, doesn't replace.

### Level 3: AI-Integrated (Automated Decision-Making in Workflow)

**What It Is:** AI makes decisions, embedded in business process

**Examples:**
- Automated fraud detection (approve/decline)
- Automated document classification
- Automated content moderation

**When to Use:**
- Decisions are routine and well-defined
- Error tolerance is acceptable
- High volume requires automation

**Cost:** ‚Ç¨150K-400K | **Time:** 8-12 weeks

**Real Example:** Our merchant category code automation (Level 3) uses GenAI + validation rules. 98% automated, 2% human review for edge cases. Processes 200K+ merchants.

### Level 4: Advanced ML (Real-Time, Complex Patterns)

**What It Is:** Deep learning, ensemble models, real-time inference

**Examples:**
- Real-time fraud detection with ensemble models
- Computer vision for quality control
- Natural language understanding for complex queries

**When to Use:**
- Patterns are complex and evolving
- Real-time decisions required
- Traditional ML insufficient

**Cost:** ‚Ç¨400K-1M+ | **Time:** 12-24 weeks

**Real Example:** Our fraud detection system (Level 4) processes 100M+ daily transactions with sub-100ms latency. Ensemble model (Random Forest + XGBoost + Neural Network) catches 99.2% of fraud.

### Level 5: Agentic AI (Autonomous Task Execution)

**What It Is:** AI agents that plan, execute, and adapt autonomously

**Examples:**
- Multi-agent systems for complex workflows
- Autonomous research agents
- Self-optimizing systems

**When to Use:**
- Tasks require planning and multi-step execution
- System needs to adapt to new situations
- Human oversight acceptable but not required for each decision

**Cost:** ‚Ç¨1M+ | **Time:** 6-24 months

**Requirements for Level 5:**
- Human-in-the-loop (HITL) for critical decisions
- Comprehensive audit logging
- Kill switch for emergency shutdown
- Extensive testing and validation

<Callout type="info" title="Default Recommendation">
Start at Level 2 or 3. Most problems don't need Level 4 or 5. You can always upgrade later if needed. Over-engineering is the #1 cause of AI project failure.
</Callout>

## The Decision Tree: Step-by-Step Evaluation

Here's the decision framework I use for every AI use case:

<Mermaid chart={`
graph TD
    A[AI Use Case Idea] --> B{Problem Real?<br/>Quantified Impact?}
    B -->|No| C[STOP: No Problem]
    B -->|Yes| D{Simpler Solution<br/>Works?}
    D -->|Yes| E[Use Simpler Solution<br/>STOP AI Work]
    D -->|No| F{Data Available?<br/>Quality >85%?}
    F -->|No| G[STOP: Collect Data First]
    F -->|Yes| H{ROI Positive?<br/>Payback &lt;18mo?}
    H -->|No| I[STOP: Not Financially Justified]
    H -->|Yes| J{AI Level Clear?<br/>Start Simple?}
    J -->|No| K[POC: Validate Approach]
    J -->|Yes| L[GO: Build to Specified Level]
    K --> M{POC Success?}
    M -->|Yes| L
    M -->|No| N[PIVOT or STOP]
`} />

### Step 1: Problem Validation

**Questions:**
- What's the actual business problem? (Not the solution)
- What's the quantified impact today?
- What does "solved" look like?
- Who owns this problem?

**Output:** 1-page problem statement with metrics

**Example:**
```
Problem: Fraudulent transactions slip through our rule-based system
Baseline: 2.8% fraud rate, costs ‚Ç¨5M annually
Success: Reduce to &lt;0.8% fraud rate (‚Ç¨3M savings)
Current process: Manual rules, updated quarterly (too slow)
Owner: Risk & Payments team
```

### Step 2: Simpler Solution Test

**Questions:**
- Can rules/heuristics solve this?
- What's the 80/20‚Äîcan 20% of rules solve 80% of the problem?
- Why hasn't simpler worked so far?

**Output:** Comparison of 3-5 simpler alternatives + why they're insufficient

**Example:**
```
Rules Only: Catches 1.8% fraud (not enough)
Hybrid (Rules + Manual Review): Catches 2.4% fraud (takes 4 hours/day)
Could we do better with rules alone? No - we've hit the ceiling
Conclusion: NEED data-driven approach
```

**If simpler works:** Use it. Stop AI work. You've saved months and money.

### Step 3: Data Reality Check

**Questions:**
- Do we HAVE the data? (Not "can we collect it")
- Is it labeled? Complete? Fresh?
- How many records? How much history?
- What's the quality?

**Output:** Data inventory spreadsheet

**Example:**
```
Data Needed: Historical transaction records with fraud labels
Data We Have: ‚úÖ 15 years of daily transactions (100M+ per day)
Labels: ‚úÖ Yes, all transactions are labeled (fraud or not)
Quality: ‚úÖ Complete, clean, continuously updated
Decision: EXCELLENT data foundation
```

**If data doesn't exist or quality is poor:** Stop. Collect data first. Come back in 6-12 months.

### Step 4: AI Level Selection

**Questions:**
- What's the simplest AI level that solves this?
- What would it cost? How long?
- Can we start simpler and upgrade later?

**Output:** 1-page recommendation (Level X, Cost Y, ROI Z)

**Example:**
```
Problem: 2.8% fraud rate, costs ‚Ç¨5M annually
Simpler (Level 0-1): Caps out at 1.8% fraud catch
Needed: Level 4 (Real-time deep learning on transaction streams)
Why: Fraud patterns evolve daily. Need adaptive, not static.
Cost-Benefit: ‚Ç¨1M initial + ‚Ç¨300K/year ops ‚Üí ‚Ç¨3M+ savings = WORTH IT
```

### Step 5: ROI Calculation

**Questions:**
- Annual benefit: What will be saved/earned?
- Implementation cost: Level-specific cost
- Operating cost: Ongoing maintenance/infrastructure
- Payback period: When does it break even?
- Risk adjustment: What if it's 50% as good as promised?

**Output:** ROI spreadsheet

**Example:**
```
ROI CALCULATION: Fraud Detection (Level 4)

Year 1 (Implementation Year):
‚îú‚îÄ Implementation cost: ‚Ç¨600K
‚îú‚îÄ Infrastructure cost: ‚Ç¨200K
‚îú‚îÄ Team cost (2 FTE): ‚Ç¨250K
‚îî‚îÄ Total Year 1 cost: ‚Ç¨1,050K

Annual Benefit (Ongoing):
‚îú‚îÄ Fraud reduction: ‚Ç¨2M/year (0.8% rate instead of 2.8%)
‚îú‚îÄ Manual review savings: ‚Ç¨300K/year
‚îî‚îÄ Total annual benefit: ‚Ç¨2.3M/year

Payback Period: Year 1 = -‚Ç¨1,050K + ‚Ç¨2.3M = +‚Ç¨1.25M
‚Üí Positive in Year 1. ‚úÖ GO

Risk Scenario (50% as good):
‚îú‚îÄ Fraud reduction: ‚Ç¨1M/year
‚îú‚îÄ Manual review savings: ‚Ç¨150K/year
‚îú‚îÄ Total benefit: ‚Ç¨1.15M/year
‚îú‚îÄ Payback: 0.9 years
‚Üí Still positive. ‚úÖ GO
```

**If ROI is negative or payback >2 years:** Stop. Not financially justified.

### Step 6: Go/No-Go Decision

**Decision Matrix:**

| Problem? | Simpler Works? | Data Available? | ROI Positive? | DECISION |
|----------|----------------|-----------------|---------------|----------|
| YES | NO | YES | YES | ‚úÖ **GO** - Build to AI level specified |
| YES | NO | UNCLEAR | YES | üü° **POC** - Run 2-4 week PoC to validate data |
| YES | NO | NO | YES | üõë **STOP** - Collect data first (or use Level 0-1) |
| YES | YES | - | - | ‚úÖ **GO** - Use simpler solution, stop |
| NO | - | - | - | üõë **STOP** - No real problem |
| YES | NO | YES | NO | üõë **STOP** - Not financially justified |

## Real Examples: Three Production Use Cases

Let me show you how this framework applies to real projects I've worked on:

### Example 1: Real-Time Fraud Detection ‚úÖ Production (2+ years)

**Problem:** A payment processing network handles 100M+ daily payment transactions. Rule-based fraud detection had high false positive rates‚Äîlegitimate transactions were being declined.

**Assessment:**

**Desirability:** ‚úÖ
- Current cost: ‚Ç¨5M annually in fraud losses
- Target: Reduce to &lt;0.8% fraud rate (‚Ç¨3M savings)
- Success metric: 99%+ fraud catch rate, &lt;0.5% false positives

**Feasibility:** ‚úÖ
- Data: 15+ years of transaction history, 100M+ daily transactions
- Labels: All transactions labeled (fraud or not)
- Quality: Complete, clean, continuously updated
- Skills: Team had ML expertise

**Viability:** ‚úÖ
- ROI: ‚Ç¨2M+/year savings vs ‚Ç¨1M initial + ‚Ç¨300K/year ops
- Payback: Positive in Year 1
- Team: Risk management team committed

**AI Level:** Level 4 (Advanced ML)
- Real-time transaction scoring (sub-100ms latency)
- Ensemble model: Random Forest + XGBoost + Neural Network
- Monthly retraining as fraud patterns evolve

**Result:** 99.2% fraud catch rate, 40% reduction in false positives, ‚Ç¨2M+/year savings. Production for 2+ years, continuously improving.

**Key Lesson:** This took 2 years to mature. Started with simple Logistic Regression (85% accuracy), evolved to ensemble models. Don't expect Level 4 perfection on day one.

### Example 2: Merchant Category Code Automation ‚úÖ Production (6 months)

**Problem:** Every merchant in a payment processing network must be assigned a Merchant Category Code (MCC). Manual process took 2-3 days per merchant, high error rate.

**Assessment:**

**Desirability:** ‚úÖ
- Current cost: 2-3 days per merchant, ‚Ç¨15 per merchant
- Target: &lt;2 hours per merchant, ‚Ç¨3 per merchant
- Success metric: 98%+ automation rate

**Feasibility:** ‚úÖ
- Data: 200K+ merchant records, business descriptions
- Issue: Historical data 80% correct (can't train ML on 20% wrong labels)
- Pivot: Use GenAI + validation rules instead of custom ML

**Viability:** ‚úÖ
- ROI: 80% time savings, 100% compliance
- Cost: ‚Ç¨50K over 6 months
- Payback: Immediate (time savings)

**AI Level:** Level 3 (AI-Integrated)
- GenAI prompt: "Based on this business description, what's the correct MCC code?"
- Validation rules for compliance
- 98% automated, 2% human review for edge cases

**Result:** 98% fully automated, 80% time savings (2-3 days ‚Üí &lt;2 hours), 100% compliance accuracy.

**Key Lesson:** Initially planned custom ML classifier. GenAI + rules worked better, faster to deploy, fully explainable. Sometimes hybrid beats pure ML.

### Example 3: Customer Churn Prediction üü° PoC (Decision Pending)

**Problem:** Banking platform clients need to identify customers at risk of leaving before they switch banks.

**Assessment:**

**Desirability:** ‚úÖ
- Current: Only find out when customers close accounts (too late)
- Target: Early identification ‚Üí targeted retention offers
- Success metric: 15-20% improvement in retention

**Feasibility:** üü°
- Data: 6-12 months per customer (minimum viable)
- Issue: Some regions have only 3 months of history
- Issue: Definition of "churn" varies by product
- Issue: Data quality gaps between banks

**Viability:** üü°
- ROI: Estimated 15-20% retention improvement
- Cost: ‚Ç¨50K for PoC, ‚Ç¨200K+ for full build
- Payback: Year 1.5 (if PoC succeeds)

**AI Level:** Level 2-3 (Predictive Analytics)
- Baseline: Simple rule "No login in 3 months" = 72% accuracy
- ML Model: Gradient Boosting (XGBoost) with 15-20 features
- Target: 82-85% accuracy (beats baseline by 10+ points)

**Status:** PoC Week 3 of 4. Initial validation: 81% accuracy (beats 72% baseline). Data quality issues discovered. Decision gate this week: GO, PIVOT, or STOP.

**Key Lesson:** This is what a real PoC looks like. 4 weeks, clear success criteria, decision gates. ‚Ç¨50K to answer a ‚Ç¨5M question is a good investment. Data quality issues are real‚Äîbetter to discover them in PoC than after 6 months of development.

## PoC Validation Checklist

If you're uncertain about any dimension, run a 2-4 week PoC. Here's the checklist:

### PoC Success Criteria (Define BEFORE Starting)

- [ ] **Model Accuracy:** ‚â•75% (or beats baseline by X points)
- [ ] **Data Quality:** Acceptable for production (completeness, accuracy)
- [ ] **Team Readiness:** Can operationalize this
- [ ] **ROI Math Holds:** Actual results match projections
- [ ] **Technical Feasibility:** Can integrate with existing systems

### PoC Decision Points

**All criteria met?** ‚Üí ‚úÖ GO to full build

**1-2 criteria missed?** ‚Üí üîÑ PIVOT (change approach, simplify, extend PoC)

**3+ criteria missed?** ‚Üí üõë STOP (not viable right now)

### PoC Structure (4 Weeks)

**Week 1:** Data collection and quality assessment
- [ ] Extract and validate data
- [ ] Assess completeness and accuracy
- [ ] Identify gaps and blockers

**Week 2:** Baseline model
- [ ] Build simple rule-based baseline
- [ ] Measure baseline performance
- [ ] Set target to beat

**Week 3:** ML model development
- [ ] Feature engineering
- [ ] Model training and validation
- [ ] Performance comparison vs baseline

**Week 4:** Decision gate
- [ ] Review all findings
- [ ] Compare against success criteria
- [ ] Make GO/PIVOT/STOP decision

**Cost:** ‚Ç¨50K-100K for 4-week PoC

**Value:** Answers "Is this solvable?" before committing to full build (‚Ç¨200K-1M+)

## Common Mistakes (And How to Avoid Them)

### Mistake 1: "The Data is Terrible"

**Finding:** "We have data, but quality is 60% (too many missing values)"

**What NOT to Do:** Build Level 4 hoping ML can fix bad data

**What TO Do:**
1. **STOP** - Come back when data is clean (6-12 months?)
2. **PIVOT** - Use a different approach (rules + manual instead of ML)
3. **GO WITH CAUTION** - Build Level 1-2 model that's tolerant of bad data

### Mistake 2: "Simpler Works, Just Not Perfectly"

**Finding:** "We can get 85% of the problem solved with better rules"

**What NOT to Do:** Build AI when rules already work

**What TO Do:**
1. **STOP AI work** - Rules solve 85%, maybe that's good enough?
2. **POC** - 2-week PoC to see if AI can get to 92% and if it's worth 3x the cost
3. **GO** - AI gets us from 85% to 95%, ROI justifies it

### Mistake 3: "ROI is Marginal"

**Finding:** "Benefit is ‚Ç¨100K/year, but cost is ‚Ç¨200K + ‚Ç¨50K/year to operate"

**What NOT to Do:** Build because "AI is cool" when ROI doesn't work

**What TO Do:**
1. **STOP** - Payback is 2 years, not worth the risk
2. **POC** - Test if you can achieve the benefit with cheaper approach
3. **PIVOT** - Can we reduce cost and still solve the problem?

### Mistake 4: "We're Uncertain"

**Finding:** "We THINK this could work, but we're not sure"

**What NOT to Do:**
- STOP because you're uncertain
- GO blindly when you're uncertain

**What TO Do:**
- üü° **POC** - Run a small, time-bound PoC (2-4 weeks) to reduce uncertainty

## The AI Architecture Gate

For enterprise organizations, I recommend implementing an **AI Architecture Gate**‚Äîa mandatory review before any AI project gets budget approval.

### Gate Structure

**Gate 1: Problem Validation**
- [ ] Clear business owner
- [ ] Measurable success metric
- [ ] Non-AI solution considered

**Gate 2: AI Necessity**
- [ ] Why rules are insufficient
- [ ] Why heuristics fail
- [ ] Why AI adds value

**Gate 3: AI Level Approval**
- [ ] Approved level: Assisted / Integrated / Driven / Agentic
- [ ] Agentic AI requires: HITL, audit logging, kill switch

**Gate 4: Data & Compliance**
- [ ] Data source approved
- [ ] GDPR / regulatory review done
- [ ] Security sign-off

**Gate 5: Risk Assessment**
- [ ] Error tolerance defined
- [ ] Fallback strategy defined
- [ ] Monitoring KPIs agreed

**Final Decision:**
- [ ] Approved to build
- [ ] Revise & resubmit
- [ ] Rejected (Non-AI solution recommended)

This gate ensures only justified, feasible, and safe AI use cases move into implementation.

## Practical Tools & Templates

### 1. 3-Dimensional Assessment Worksheet

Download: [AI Use Case Assessment Worksheet](/templates/ai-use-case-assessment-worksheet.md)

**Sections:**
- Desirability scoring (1-10 for each criterion)
- Feasibility checklist (data, skills, compliance)
- Viability calculation (ROI, payback, risk)
- Overall recommendation (GO / POC / STOP)

### 2. ROI Calculator Template

Download: [AI ROI Calculator](/templates/ai-roi-calculator.md)

**Includes:**
- Implementation cost breakdown
- Annual benefit calculation
- Operating cost estimation
- Payback period analysis
- Risk-adjusted scenarios

### 3. PoC Validation Checklist

Download: [PoC Validation Checklist](/templates/poc-validation-checklist.md)

**Includes:**
- Success criteria definition
- Week-by-week PoC structure
- Decision gate framework
- Go/Pivot/Stop criteria

### 4. AI Level Decision Matrix

Download: [AI Level Decision Matrix](/templates/ai-level-decision-matrix.md)

**Helps you:**
- Understand each AI level
- Match level to problem complexity
- Estimate cost and timeline
- Avoid over-engineering

### 5. Decision Tree Flowchart

Download: [AI Use Case Decision Tree](/templates/ai-decision-tree.md)

**Visual guide:**
- Step-by-step evaluation process
- Decision points and criteria
- Go/No-Go gates
- POC triggers

## Checklist: Are You Ready to Build?

Before you move beyond evaluation, answer ALL of these:

- [ ] Problem is REAL (quantified impact, clear owner)
- [ ] Simpler solutions INSUFFICIENT (tested 3-5 alternatives)
- [ ] Data EXISTS and is CLEAN (quality >85%, labeled, accessible)
- [ ] AI Level is CLEAR (start simple, can upgrade later)
- [ ] ROI is POSITIVE (payback &lt;18 months, risk-adjusted)
- [ ] Stakeholders AGREED (business owner, technical lead, finance)
- [ ] Budget is APPROVED (build + operations)
- [ ] Team is ASSIGNED (has or can acquire skills)

**If ANY is unchecked:** Don't proceed. Resolve it first.

## Key Takeaways

1. **Start with the problem, not the solution** - "We want AI" isn't a problem statement.

2. **Test simpler first** - Rules and heuristics solve 80% of problems. Don't jump to AI.

3. **Check data early** - It's the biggest blocker. "Can we collect it?" is different from "Do we have it?"

4. **Calculate real ROI** - Not theoretical savings. Include implementation, operations, and risk.

5. **Match AI level to problem** - Start simple (Level 2-3). Upgrade later if needed.

6. **Use PoCs for uncertainty** - ‚Ç¨50K to answer a ‚Ç¨5M question is smart.

7. **Embrace NO decisions** - They're success, not failure. You've saved months and money.

8. **The goal isn't to build** - The goal is to answer: "Does this problem actually need AI?"

## What's Next

Now that you've evaluated your use case, here's where to go:

- **If GO:** Start with [The Anatomy of a Production LLM Call](/blog/the-anatomy-of-a-production-llm-call) to understand the fundamentals
- **If POC:** Use the PoC checklist above and plan your 4-week validation
- **If STOP:** Document why, revisit in 6 months when conditions may have changed

For technical implementation guides:
- [RAG Fundamentals](/blog/rag-fundamentals-making-llms-trustworthy) - When you need external knowledge
- [Building Your First AI Agent](/blog/building-your-first-ai-agent) - When you need autonomous systems
- [Red Teaming AI Systems](/blog/red-teaming-ai-systems) - Before you deploy

<Callout type="info" title="Want Help Evaluating Your Use Case?">
I offer office hours for teams evaluating AI use cases. Book a session to walk through the framework with your specific problem. [Contact me](/contact) to schedule.
</Callout>

---

*What AI use cases are you evaluating? I'd love to hear about your experiences. Connect with me on [LinkedIn](https://www.linkedin.com/in/praveensrinagy) or [reach out](/contact) directly.*

