---
title: "Before You Build: A Realistic Framework for Evaluating AI Use Cases"
description: "Why 80% of AI projects fail and how to avoid being one of them. A practitioner's framework for evaluating AI use cases before you write a single line of code."
date: 2026-01-06
tags: ["AI", "Architecture", "Best Practices"]
pinned: true
---

import Callout from '../../components/Callout.astro';
import Mermaid from '../../components/Mermaid.astro';
import BlogDisclaimer from '../../components/BlogDisclaimer.astro';
import YouTubeEmbed from '../../components/YouTubeEmbed.astro';

Imagine this scenario: At 2 AM on a Tuesday, a team gets a call. Their AI-powered fraud detection system has flagged 40% of legitimate transactions as fraudulent. Customers are furious. The system has been in production for three months, and they've just discovered a fundamental flaw: they'd never properly validated whether AI was the right solution.

That night cost them ‚Ç¨50K in lost revenue and three months of development time. The lesson? **Most AI projects fail not because the technology is wrong, but because the use case evaluation is wrong.**

[McKinsey's 2025 State of AI report](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai) ([detailed analysis](https://medium.com/@david.hung.yang/deep-dive-into-mckinseys-the-state-of-ai-in-2025-from-everyone-using-ai-to-a-few-using-it-6095987cec14)) finds that while 88% of organizations use AI in at least one business function, nearly two-thirds remain in experiment or pilot mode, with only about one-third having genuinely scaled AI across functions. Why? They skip the assessment phase. They jump to building before asking "Should we build this?"

This is the framework I wish we'd used back then. I apply it to every AI initiative now, before writing a single line of code.

<Callout type="warning" title="The Real Question">
This isn't about "How do we build an AI/ML model?" It's about "Does this problem actually NEED AI? And if yes, what LEVEL of AI?"
</Callout>

<h2 id="three-failures-that-kill-ai-projects">The Three Failures That Kill AI Projects</h2>

Why do AI projects fail? After working on dozens of initiatives, I've noticed three patterns that keep showing up:

1. **Bad Problem Statement** - "We want to use AI for customer support" isn't a problem-it's a solution looking for a problem. What's the actual pain? Long response times? High ticket volume? Start with the business problem, not the technology.

2. **Wrong Abstraction Level** - Building a Level 4 (Advanced ML) system when Level 1 (Rules) would work. Over-engineering kills projects. A simple rule-based system catches 85% of cases, but teams jump to deep learning "because AI is cool." Match the AI level to the problem complexity.

3. **Wrong Expectations** - Expecting 100% accuracy from day one. AI systems are probabilistic. They improve over time. Teams abandon projects when initial accuracy is 75% instead of 95%. Set realistic success criteria based on baseline performance.

<YouTubeEmbed id="q4T6lA2Kn4s" title="Before You Build: A Realistic Framework for Evaluating AI Use Cases" />

<Callout type="success" title="Real Example">
A fraud detection system started at 85% accuracy with simple Logistic Regression. After two years of iteration, it reached 99.2% with ensemble models. But they almost killed it in month three because "85% wasn't good enough." The lesson: start simple, improve iteratively.
</Callout>

<h2 id="3-dimensional-assessment-framework">The 3-Dimensional Assessment Framework</h2>

Every AI use case must pass three dimensions: **Desirability**, **Feasibility**, and **Viability**. Fail any dimension, and the project should stop or pivot.

<img id="3d-assessment-framework" src="/images/3d-framework.png" alt="3-Dimensional Assessment Framework: Desirability Check ‚Üí Feasibility Check ‚Üí Viability Check" />

<h3 id="dimension-1-desirability">Dimension 1: Desirability - Is the Problem Worth Solving?</h3>

**Question:** Would solving this problem create measurable business value?

**What to Assess:**

1. **Quantified Impact**
   - What's the current cost of the problem? (Time, money, errors)
   - What's the cost of doing nothing?
   - What's the measurable improvement we need?

2. **Strategic Alignment**
   - Does this align with business priorities?
   - Is there executive sponsorship?
   - Will users actually adopt this?

3. **Success Metrics**
   - How will we measure success?
   - What's the baseline performance today?
   - What improvement justifies the investment?

**Example: Fraud Detection**

```
Problem: Fraudulent transactions slip through our rule-based system
Current State: 2.8% fraud rate, costs ‚Ç¨5M annually
Target State: Reduce to &lt;0.8% fraud rate (‚Ç¨3M savings)
Baseline: Manual rules catch 1.8% fraud
Success Metric: Fraud catch rate >99%, false positives &lt;0.5%
```

**Red Flags:**
- ‚ùå Vague problem statement ("improve customer experience")
- ‚ùå No baseline metrics
- ‚ùå No clear business owner
- ‚ùå Success criteria are subjective

**Green Lights:**
- ‚úÖ Quantified current cost
- ‚úÖ Clear target improvement
- ‚úÖ Measurable success metrics
- ‚úÖ Business owner identified

<h3 id="dimension-2-feasibility">Dimension 2: Feasibility - Can We Technically Do This?</h3>

**Question:** Do we have the data, skills, and infrastructure to build this?

**What to Assess:**

1. **Data Reality Check**
   - Do we have the data we need? (Not "can we collect it"-do we HAVE it?)
   - Is the data labeled? Complete? Fresh?
   - How much historical data exists?
   - What's the data quality?

2. **Technical Fit**
   - Does this problem require AI, or would rules/heuristics work?
   - Do we have the technical skills in-house?
   - Can we integrate with existing systems?
   - Are there compliance/regulatory constraints?

3. **Data Access & Governance**
   - Can we legally use this data?
   - Do we have privacy/compliance approval?
   - Who owns the data, and will they give us access?

**Example: Customer Churn Prediction**

```
Data Needed: 6-12 months of customer behavior + churn labels
Data We Have: ‚úÖ Yes, 6-12 months per customer
Labels: ‚úÖ Yes, but...
Issue: Some regions have only 3 months of history
Issue: Definition of "churn" varies by product
Decision: DATA EXISTS, but quality needs validation (PoC risk)
```

**Example: Merchant Category Code Automation**

```
Data Needed: Merchant records with correct category codes
Data We Have: ‚úÖ Yes, 200K merchant records
Issue: Historical data is 80% correct (20% wrong categories)
Issue: No machine-readable explanations of why merchants get certain codes
Decision: Can't train ML on 80% correct labels. STOP or pivot to rules + GenAI
```

**Red Flags:**
- ‚ùå Data doesn't exist (only "we could collect it")
- ‚ùå Data quality &lt;70% (too many missing values, errors)
- ‚ùå No labeled training data
- ‚ùå Compliance blockers (GDPR, industry regulations)
- ‚ùå Data locked in vendor systems we don't control

**Green Lights:**
- ‚úÖ Data exists and is accessible
- ‚úÖ Data quality >85%
- ‚úÖ Labeled training data available
- ‚úÖ Compliance approval obtained
- ‚úÖ Technical team has required skills

<h3 id="dimension-3-viability">Dimension 3: Viability - Can We Sustain This?</h3>

**Question:** Is this financially justified and operationally sustainable?

**What to Assess:**

1. **ROI Calculation**
   - Annual benefit: What will be saved/earned?
   - Implementation cost: What will it cost to build?
   - Operating cost: Ongoing maintenance/infrastructure
   - Payback period: When does it break even?

2. **Team & Skills**
   - Do we have the right team?
   - Can we maintain this long-term?
   - What training is needed?

3. **Change Management**
   - Will users adopt this?
   - What process changes are required?
   - Is the organization ready?

**Example: Fraud Detection ROI**

```
Year 1 (Implementation):
‚îú‚îÄ Implementation cost: ‚Ç¨600K
‚îú‚îÄ Infrastructure cost: ‚Ç¨200K
‚îú‚îÄ Team cost (2 FTE): ‚Ç¨250K
‚îî‚îÄ Total Year 1 cost: ‚Ç¨1,050K

Annual Benefit (Ongoing):
‚îú‚îÄ Fraud reduction: ‚Ç¨2M/year (0.8% rate instead of 2.8%)
‚îú‚îÄ Manual review savings: ‚Ç¨300K/year
‚îî‚îÄ Total annual benefit: ‚Ç¨2.3M/year

Payback Period: Year 1 = -‚Ç¨1,050K + ‚Ç¨2.3M = +‚Ç¨1.25M
‚Üí Positive in Year 1. ‚úÖ GO

Risk Scenario (50% as good):
‚îú‚îÄ Fraud reduction: ‚Ç¨1M/year
‚îú‚îÄ Manual review savings: ‚Ç¨150K/year
‚îú‚îÄ Total benefit: ‚Ç¨1.15M/year
‚îú‚îÄ Payback: 0.9 years
‚Üí Still positive. ‚úÖ GO
```

**Red Flags:**
- ‚ùå Payback period >2 years
- ‚ùå ROI is negative even in best case
- ‚ùå No budget for ongoing operations
- ‚ùå Team doesn't have skills (and can't acquire them)
- ‚ùå Users are resistant to change

**Green Lights:**
- ‚úÖ Positive ROI in Year 1
- ‚úÖ Payback period &lt;18 months
- ‚úÖ Budget approved for build and operations
- ‚úÖ Team has or can acquire skills
- ‚úÖ Users are engaged and supportive

<h2 id="5-levels-of-ai">The 5 Levels of AI: From Analytics to Agentic AI</h2>

Not all AI is created equal. Understanding AI levels helps you pick the right solution for your problem and avoid over-engineering.

<img id="5-levels-of-ai-diagram" src="/images/ai-levels-diagram.png" alt="The 5 Levels of AI: Level 0 (No AI Needed) through Level 5 (Agentic AI)" />

<div class="table-wrapper">

| Level | What It Is | When to Use | Cost | Time | Example |
|-------|------------|-------------|------|------|---------|
| **0** | Rule-based logic, heuristics | Deterministic problems, rules capture all cases | ‚Ç¨5K-20K | 2 weeks | "If amount > ‚Ç¨5K, flag for review" |
| **1** | Statistical models, regression | Linear relationships, historical patterns | ‚Ç¨20K-50K | 3 weeks | Sales forecasting, customer segmentation |
| **2** | AI suggests, human decides | Human judgment critical, low error tolerance | ‚Ç¨50K-150K | 4-6 weeks | Churn prediction: AI flags at-risk customers, team decides offers |
| **3** | AI makes decisions, automated | Routine decisions, acceptable errors, high volume | ‚Ç¨150K-400K | 8-12 weeks | Merchant code automation: 98% automated, GenAI + rules |
| **4** | Deep learning, ensemble models | Complex evolving patterns, real-time required | ‚Ç¨400K-1M+ | 12-24 weeks | Fraud detection: 100M+ daily transactions, 99.2% catch rate |
| **5** | Autonomous agents, multi-agent systems | Planning + execution, adaptive systems | ‚Ç¨1M+ | 6-24 months | Multi-agent workflows (requires HITL, audit logging, kill switch) |

</div>

<Callout type="info" title="Default Recommendation">
Start at Level 2 or 3. Most problems don't need Level 4 or 5. You can always upgrade later if needed. Over-engineering is a leading cause of AI project failure-[McKinsey's research](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai) ([analysis](https://medium.com/@david.hung.yang/deep-dive-into-mckinseys-the-state-of-ai-in-2025-from-everyone-using-ai-to-a-few-using-it-6095987cec14)) shows that most organizations remain stuck in pilot mode, often because they've over-engineered solutions instead of starting simple.
</Callout>

For detailed guidance on selecting the right level, download the [AI Level Decision Matrix](/templates/ai-use-cases/ai-level-decision-matrix).

<h2 id="decision-tree-quick-reference">The Decision Tree: Quick Reference</h2>

The framework above covers the detailed assessment. If you need a quick reference, here's the decision flow:

<img id="decision-tree-diagram" src="/images/decision-tree.png" alt="The Decision Tree" />


**Decision Matrix:**

<div class="table-wrapper">

| Problem? | Simpler Works? | Data Available? | ROI Positive? | DECISION |
|----------|----------------|-----------------|---------------|----------|
| YES | NO | YES | YES | ‚úÖ **GO** - Build to AI level specified |
| YES | NO | UNCLEAR | YES | üü° **POC** - Run 2-4 week PoC to validate data |
| YES | NO | NO | YES | üõë **STOP** - Collect data first (or use Level 0-1) |
| YES | YES | - | - | ‚úÖ **GO** - Use simpler solution, stop |
| NO | - | - | - | üõë **STOP** - No real problem |
| YES | NO | YES | NO | üõë **STOP** - Not financially justified |

</div>

For detailed step-by-step evaluation, use the [AI Use Case Assessment Worksheet](/templates/ai-use-cases/ai-use-case-assessment-worksheet).

<h2 id="examples-two-use-cases">Examples: Two Use Cases</h2>

Here's how this framework played out in two real projects:

<h3 id="example-1-real-time-fraud-detection">Example 1: Real-Time Fraud Detection</h3>

- **Problem:** Payment processing network handles 100M+ daily transactions. Rule-based fraud detection had high false positive rates - legitimate transactions were being declined.
- **Assessment:**
  - ‚úÖ Desirability (‚Ç¨5M annual cost ‚Üí ‚Ç¨3M savings target)
  - ‚úÖ Feasibility (15+ years of labeled data, ML expertise)
  - ‚úÖ Viability (‚Ç¨2M+/year savings, positive Year 1 ROI)
- **AI Level:** Level 4 (Advanced ML) ‚Äì Real-time ensemble model, sub-100ms latency
- **Result:** 99.2% fraud catch rate, 40% reduction in false positives, ‚Ç¨2M+/year savings
- **Key Lesson:** This took two years to mature. They started with Logistic Regression at 85% accuracy, then evolved to ensemble models. Don't expect Level 4 perfection on day one - it doesn't work that way.

<h3 id="example-2-customer-churn-prediction">Example 2: Customer Churn Prediction</h3>

- **Problem:** Banking platform needs to identify at-risk customers before they switch banks.
- **Assessment:**
  - ‚úÖ Desirability: Early identification ‚Üí retention offers
  - üü° Feasibility: Data quality gaps between banks, varying definitions
  - üü° Viability: ‚Ç¨50K PoC, ‚Ç¨200K+ full build, Year 1.5 payback
- **AI Level:** Level 2-3 (Predictive Analytics)
  - Baseline: 72% accuracy
  - Target: 82-85% accuracy
- **Status:** 
  - PoC Week 3 of 4
  - Initial validation: 81% accuracy (beats 72% baseline)
  - Data quality issues discovered
  - Decision gate: GO, PIVOT, or STOP
- **Key Lesson:** 
  - This is what a real PoC looks like: Four weeks, clear success criteria, and decision gates.
  - Spending ‚Ç¨50K to answer a ‚Ç¨5M question is smart.
  - Data quality issues are real - it's much better to discover them in a PoC than after six months of development.

<h2 id="poc-validation-when-uncertainty-exists">PoC Validation: When Uncertainty Exists</h2>

If you're uncertain about any dimension, run a 2-4 week PoC. **Define success criteria BEFORE starting:**

- Model accuracy ‚â•75% (or beats baseline by X points)
- Data quality acceptable for production
- Team can operationalize this
- ROI math holds (actual results match projections)
- Technical feasibility confirmed

**Decision Points:**
- All criteria met? ‚Üí ‚úÖ GO to full build
- Missed 1-2 criteria? ‚Üí üîÑ PIVOT (change approach, simplify)
- Missed 3+ criteria? ‚Üí üõë STOP (not viable right now)

**Structure:** Week 1 (data assessment) ‚Üí Week 2 (baseline) ‚Üí Week 3 (ML model) ‚Üí Week 4 (decision gate)

**Cost:** ‚Ç¨50K-100K for a 4-week PoC. **Value:** It answers "Is this solvable?" before you commit ‚Ç¨200K-1M+ to a full build.

For the complete PoC framework, download the [PoC Validation Checklist](/templates/ai-use-cases/poc-validation-checklist).

<h2 id="common-mistakes-and-how-to-avoid-them">Common Mistakes (And How to Avoid Them)</h2>

1. **"The Data is Terrible"** - Data quality is 60% but building Level 4 hoping ML can fix it. **Fix:** STOP and clean data first, or PIVOT to rules + manual, or GO WITH CAUTION with Level 1-2 models tolerant of bad data.

2. **"Simpler Works, Just Not Perfectly"** - Rules solve 85% of the problem. **Fix:** Maybe 85% is good enough? Or run a PoC to see if AI gets to 92% and if it's worth 3x the cost.

3. **"ROI is Marginal"** - Benefit is ‚Ç¨100K/year, cost is ‚Ç¨200K + ‚Ç¨50K/year. **Fix:** STOP (payback >2 years), or POC to test cheaper approach, or PIVOT to reduce costs.

4. **"We're Uncertain"** - Think it could work but not sure. **Fix:** Run a 2-4 week PoC. Don't STOP because uncertain, don't GO blindly. Use PoC to reduce uncertainty.

<h2 id="ai-architecture-gate">The AI Architecture Gate</h2>

For enterprise organizations, implement an **AI Architecture Gate**-a mandatory review before any AI project gets budget approval. Five gates: Problem Validation ‚Üí AI Necessity ‚Üí AI Level Approval ‚Üí Data & Compliance ‚Üí Risk Assessment.

The goal? Only justified, feasible, and safe AI use cases get budget approval. Download the [AI Architecture Gate template](/templates/ai-use-cases/ai-architecture-gate) for the complete framework.

<h2 id="practical-tools-templates">Practical Tools & Templates</h2>

You can find all templates on the [Templates page](/templates) with descriptions and download options.

<h3 id="3-dimensional-assessment-worksheet">1. 3-Dimensional Assessment Worksheet</h3>

Download: [AI Use Case Assessment Worksheet](/templates/ai-use-cases/ai-use-case-assessment-worksheet)

**Sections:**
- Desirability scoring (1-10 for each criterion)
- Feasibility checklist (data, skills, compliance)
- Viability calculation (ROI, payback, risk)
- Overall recommendation (GO / POC / STOP)

<h3 id="roi-calculator-template">2. ROI Calculator Template</h3>

Download: [AI ROI Calculator](/templates/ai-use-cases/ai-roi-calculator)

**Includes:**
- Implementation cost breakdown
- Annual benefit calculation
- Operating cost estimation
- Payback period analysis
- Risk-adjusted scenarios

<h3 id="poc-validation-checklist">3. PoC Validation Checklist</h3>

Download: [PoC Validation Checklist](/templates/ai-use-cases/poc-validation-checklist)

**Includes:**
- Success criteria definition
- Week-by-week PoC structure
- Decision gate framework
- Go/Pivot/Stop criteria

<h3 id="ai-level-decision-matrix">4. AI Level Decision Matrix</h3>

Download: [AI Level Decision Matrix](/templates/ai-use-cases/ai-level-decision-matrix)

**Helps you:**
- Understand each AI level (0-5)
- Match level to problem complexity
- Estimate cost and timeline
- Avoid over-engineering

<h3 id="ai-architecture-gate-enterprise">5. AI Architecture Gate (Enterprise)</h3>

Download: [AI Architecture Gate](/templates/ai-use-cases/ai-architecture-gate)

**For enterprise organizations:**
- 5-gate approval process
- Problem Validation ‚Üí AI Necessity ‚Üí Level Approval ‚Üí Data/Compliance ‚Üí Risk Assessment
- Sign-offs and governance
- Mandatory before budget approval

<h2 id="checklist-are-you-ready-to-build">Checklist: Are You Ready to Build?</h2>

Before moving forward, make sure you can answer all of these:

- [ ] Problem is REAL (quantified impact, clear owner)
- [ ] Simpler solutions INSUFFICIENT (tested 3-5 alternatives)
- [ ] Data EXISTS and is CLEAN (quality >85%, labeled, accessible)
- [ ] AI Level is CLEAR (start simple, can upgrade later)
- [ ] ROI is POSITIVE (payback &lt;18 months, risk-adjusted)
- [ ] Stakeholders AGREED (business owner, technical lead, finance)
- [ ] Budget is APPROVED (build + operations)
- [ ] Team is ASSIGNED (has or can acquire skills)

**If any box is unchecked:** Don't proceed. Fix it first.

<h2 id="key-takeaways">Key Takeaways</h2>

1. **Start with the problem, not the solution** - "We want AI" isn't a problem statement.

2. **Test simpler first** - Rules and heuristics solve most problems. Don't jump straight to AI. [Research shows](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai) ([detailed analysis](https://medium.com/@david.hung.yang/deep-dive-into-mckinseys-the-state-of-ai-in-2025-from-everyone-using-ai-to-a-few-using-it-6095987cec14)) that organizations starting with simpler solutions scale more successfully.

3. **Check data early** - It's the biggest blocker. "Can we collect it?" is different from "Do we have it?"

4. **Calculate real ROI** - Not theoretical savings. Include implementation, operations, and risk.

5. **Match AI level to problem** - Start simple (Level 2-3). Upgrade later if needed.

6. **Use PoCs for uncertainty** - ‚Ç¨50K to answer a ‚Ç¨5M question is smart.

7. **Embrace NO decisions** - They're success, not failure. You've saved months and money.

8. **The goal isn't to build** - The goal is to answer: "Does this problem actually need AI?"

<h2 id="whats-next">What's Next</h2>

You've evaluated your use case. What happens now depends on your decision:

- **If GO:** Grab the [AI Use Case Assessment Worksheet](/templates/ai-use-cases/ai-use-case-assessment-worksheet) and start planning implementation
- **If POC:** Use the [PoC Validation Checklist](/templates/ai-use-cases/poc-validation-checklist) to structure your 4-week validation
- **If STOP:** Document why in the assessment worksheet. Revisit in six months-conditions change

Need help with technical implementation?
- [Building Production-Ready AI Agents](/blog/building-production-ai-agents) - For autonomous systems
- [Prompt Engineering Beyond Basics](/blog/prompt-engineering-beyond-basics) - When you're ready to build

<Callout type="info" title="Want Help Evaluating Your Use Case?">
I offer office hours for teams evaluating AI use cases. Book a session to walk through the framework with your specific problem. [Contact me](/contact) to schedule.
</Callout>

---

<BlogDisclaimer />

*What AI use cases are you evaluating? I'd love to hear about your experiences. Connect with me on [LinkedIn](https://www.linkedin.com/in/praveensrinagy) or [reach out](/contact) directly.*

